"""
Train a neural network on PCSE inputs/outputs.

Example:
    %run nn/testnn.py outputs/RDMSOL -v
"""
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm

from torch import nn, optim, tensor, Tensor
from torch.utils.data import DataLoader, Dataset

import fpcup
from fpcup.typing import PathOrStr

from dataset import PCSEEnsembleDataset
from network import PCSEEmulator, device, train

### Parse command line arguments
import argparse
parser = argparse.ArgumentParser(description="Analyse a PCSE ensemble with one varying parameter, as generated by wofost_ensemble_parameters.py.")
parser.add_argument("output_dir", help="folder to load PCSE outputs from", type=fpcup.io.Path)
parser.add_argument("--results_dir", help="folder to save plots into", type=fpcup.io.Path, default=fpcup.DEFAULT_RESULTS/"sensitivity")
parser.add_argument("-n", "--number_epochs", help="number of training epochs", type=int, default=10)
parser.add_argument("-b", "--batch_size", help="batch size", type=int, default=64)
parser.add_argument("-v", "--verbose", help="increase output verbosity", action="store_true")
args = parser.parse_args()


### Constants
tag = args.output_dir.stem
lossfunc = nn.L1Loss()


### This gets executed only when the script is run normally; not by multiprocessing.
if __name__ == "__main__":
    fpcup.multiprocessing.freeze_support()

    ### SETUP
    # Data
    data = PCSEEnsembleDataset(args.output_dir)
    dataloader = DataLoader(data, batch_size=args.batch_size, shuffle=True)
    if args.verbose:
        print(f"Loaded data: {data} ; batch size {dataloader.batch_size}")

    # Network
    model = PCSEEmulator().to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)


    ### TRAINING
    losses_train, losses_test = train(model, dataloader, lossfunc, optimizer, n_epochs=args.number_epochs)


    ### PLOT
    # Constants
    epochs = np.arange(args.number_epochs) + 1
    n_batches = losses_train.shape[1]
    batches = np.arange(losses_train.size) + 1

    # Colours and labels
    batchcolour = "C2"
    epochcolour = "black"

    # Variables for limits etc.
    maxloss = max(losses_train.max(), losses_test.max())

    # Plot loss per batch
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5), layout="constrained")
    ax.plot(batches, losses_train.ravel(), color=batchcolour)

    ax.set_xlim(0, len(batches))
    ax.set_xlabel("Batch", color=batchcolour)
    ax.set_ylabel("Loss")
    ax.grid(True, axis="y", ls="--")

    # Plot loss per epoch
    ax2 = ax.twiny()
    ax2.plot(epochs, losses_train[:, -1], color=epochcolour)

    ax2.set_xlim(0, args.number_epochs)
    ax2.set_ylim(0, maxloss*1.05)
    ax2.set_xlabel("Epoch", color=epochcolour)
    ax2.grid(True, ls="--")

    # Final settings
    fig.suptitle(tag)

    # Save and close
    plt.savefig(f"nn_loss_{tag}.pdf", bbox_inches="tight")
    plt.close()
