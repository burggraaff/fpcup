"""
Train a neural network on PCSE inputs/outputs.

Example:
    %run nn/testnn.py outputs/RDMSOL -v
"""
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm

from torch import nn, optim, tensor, Tensor
from torch.utils.data import DataLoader, Dataset

import fpcup
from fpcup.typing import PathOrStr

from dataset import PCSEEnsembleDataset, PCSEEnsembleDatasetSmall
from network import PCSEEmulator, device, train

### Parse command line arguments
import argparse
parser = argparse.ArgumentParser(description="Analyse a PCSE ensemble with one varying parameter, as generated by wofost_ensemble_parameters.py.")
parser.add_argument("output_dir", help="folder to load PCSE outputs from", type=fpcup.io.Path)
parser.add_argument("--results_dir", help="folder to save plots into", type=fpcup.io.Path, default=fpcup.DEFAULT_RESULTS/"sensitivity")
parser.add_argument("-n", "--number_epochs", help="number of training epochs", type=int, default=10)
parser.add_argument("-b", "--batch_size", help="batch size", type=int, default=64)
parser.add_argument("-v", "--verbose", help="increase output verbosity", action="store_true")
args = parser.parse_args()


### Constants
tag = args.output_dir.stem
lossfunc = nn.L1Loss()


### This gets executed only when the script is run normally; not by multiprocessing.
if __name__ == "__main__":
    fpcup.multiprocessing.freeze_support()

    ### SETUP
    # Data
    data = PCSEEnsembleDatasetSmall(args.output_dir)
    dataloader = DataLoader(data, batch_size=args.batch_size, shuffle=True)
    if args.verbose:
        print(data)
        print(f"Batch size: {dataloader.batch_size}")

    # Network
    model = PCSEEmulator().to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)


    ### TRAINING
    losses_train, losses_test = train(model, dataloader, lossfunc, optimizer, n_epochs=args.number_epochs)


    ### PLOT
    fpcup.plotting.plot_loss_curve(losses_train, title=tag, saveto=f"nn_loss_{tag}.pdf")
